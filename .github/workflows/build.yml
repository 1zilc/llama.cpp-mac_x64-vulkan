name: Build llama.cpp with Vulkan on macOS x64

on:
  workflow_dispatch: # 允许手动触发
  schedule:
    - cron: "0 16 * * *" # UTC 时间每天 16:00，即北京时间凌晨 00:00

env:
  LLAMACPP_TAG: b7017

jobs:
  build-macos-vulkan:
    runs-on: macos-15-intel

    steps:
      - name: 安装依赖
        run: |
          brew install cmake libomp vulkan-headers glslang molten-vk shaderc vulkan-tools

      - name: 验证 Vulkan 安装
        run: |
          vulkaninfo --summary || true

      - name: 获取最新的 llama.cpp 标签
        run: |
          # 获取最新的 tag
          LATEST_TAG=$(gh release -R "ggml-org/llama.cpp" list --limit 1 --json tagName -q ".[0].tagName")

          # 如果获取失败，使用默认值
          if [ -z "$LATEST_TAG" ] || [ "$LATEST_TAG" = "null" ]; then
            echo "无法获取最新标签,使用${{ env.LLAMACPP_TAG }}"
          fi

          echo "标签: $LATEST_TAG"
          echo "LLAMACPP_TAG=$LATEST_TAG" >> $GITHUB_OUTPUT
          echo "LLAMACPP_TAG=$LATEST_TAG" >> $GITHUB_ENV
        env:
          GH_TOKEN: ${{ github.token }}

      - name: 检出 llama.cpp 代码
        uses: actions/checkout@v4
        with:
          repository: ggml-org/llama.cpp
          ref: ${{ env.LLAMACPP_TAG }}

      - name: 设置 ccache
        uses: hendrikmuhs/ccache-action@v1.2
        with:
          key: ${{ runner.os }}-ccache-${{ env.LLAMACPP_TAG }}
          restore-keys: |
            ${{ runner.os }}-ccache-latest
          create-symlink: true

      - name: 配置 CMake (Vulkan 支持)
        run: |
          cmake -B build \
            -DGGML_METAL=OFF \
            -DGGML_VULKAN=ON \
            -DVulkan_LIBRARY=$(brew --prefix molten-vk)/lib/libMoltenVK.dylib \
            -DVulkan_INCLUDE_DIR=$(brew --prefix molten-vk)/include \
            -DCMAKE_BUILD_TYPE=Release \
            -DCMAKE_INSTALL_RPATH="@executable_path" \
            -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON \
            -DGGML_RPC=ON \
            -DCMAKE_OSX_DEPLOYMENT_TARGET=13.3 \
            -DCMAKE_C_COMPILER_LAUNCHER=ccache \
            -DCMAKE_CXX_COMPILER_LAUNCHER=ccache \

      - name: 编译
        run: |
          cmake --build build --config Release -j$(sysctl -n hw.ncpu)

      - name: 显示构建信息
        run: |
          ls -lh build/bin/ || ls -lh build/
          file build/bin/llama-cli || file build/llama-cli || true

      - name: 打包构建产物
        run: |
          cd build
          mkdir -p llama.cpp-vulkan-macos

          # 复制二进制文件
          if [ -d "bin" ]; then
            cp -r bin llama.cpp-vulkan-macos/
          else
            mkdir -p llama.cpp-vulkan-macos/bin
            cp llama-* llama.cpp-vulkan-macos/bin/ 2>/dev/null || true
          fi

          # 复制必要的库文件
          cp $(brew --prefix molten-vk)/lib/libMoltenVK.dylib llama.cpp-vulkan-macos/bin/ 2>/dev/null || true

          install_name_tool -change /usr/local/opt/molten-vk/lib/libMoltenVK.dylib \
          ./libMoltenVK.dylib llama.cpp-vulkan-macos/bin/libggml-vulkan.dylib

          # 打包
          tar -czf llama-${{ env.LLAMACPP_TAG }}-bin-macos-x64.tar.gz llama.cpp-vulkan-macos

      - name: 上传构建产物
        uses: actions/upload-artifact@v4
        with:
          name: llama.cpp-vulkan-macos
          path: build/llama-${{ env.LLAMACPP_TAG }}-bin-macos-x64.tar.gz
          retention-days: 30

      - name: 创建 Release
        uses: softprops/action-gh-release@v1
        with:
          tag_name: ${{ env.LLAMACPP_TAG }}
          name: ${{ env.LLAMACPP_TAG }}
          body: https://github.com/ggml-org/llama.cpp/releases/tag/${{ env.LLAMACPP_TAG }}
          files: build/llama-${{ env.LLAMACPP_TAG }}-bin-macos-x64.tar.gz
          draft: false
          prerelease: false
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
